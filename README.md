# Rag-Based-Document-Chatbot

**1. Source Code Documentation**

Overview: 
This chatbot implements a Retrieval-Augmented Generation (RAG) framework using Google's Gemini-1.5-Pro model. It allows users to upload PDFs, extracts text, converts it into vector embeddings, and retrieves contextually relevant information to answer user queries.

Key Functionalities:
i)	PDF Processing
  o	Extracts text from uploaded PDFs.
  o	Splits text into manageable chunks for efficient retrieval.

ii) Vector Store (FAISS) Creation
  o	Converts text into embeddings using GoogleGenerativeAIEmbeddings.
  o	Stores vectors using FAISS for efficient retrieval.

iii) Query Processing & Response Generation
  o	Uses similarity search in FAISS to find relevant text.
  o	Passes retrieved content to Gemini-1.5-Pro for answer generation.

iv) Streamlit UI
  o	Provides an interactive interface for users to upload PDFs and ask questions.

**2. Instructions for Running the Chatbot**

Step 1: Install Required Libraries
    Run the following command in your terminal:
      Code:  pip install streamlit PyPDF2 langchain langchain_google_genai google-generativeai faiss-cpu

Step 2: Obtain a Google API Key
    â€¢	Visit MakerSuite to generate your API key.

Step 3: Run the Chatbot
    Save the provided script as chatbot.py and execute:
      Code: streamlit run chatbot.py

Step 4: Interact with the Chatbot
  1.	Enter your Google API key.
  2.	Upload one or more PDF files.
  3.	Click "Submit & Process" to generate the knowledge base.
  4.	Ask questions, and the chatbot will return relevant answers.

**3. Tech Stack Choices**
ðŸ”¹ Programming Language
    â€¢	Python â€“ Ideal for AI-based applications due to its rich ecosystem of ML and NLP libraries.
ðŸ”¹ Frontend & UI
    â€¢	Streamlit â€“ Used for building the web-based UI for document upload and interaction.
ðŸ”¹ Document Processing
    â€¢	PyPDF2 â€“ Extracts text from PDFs.
ðŸ”¹ AI & NLP
    â€¢	Google Gemini API (ChatGoogleGenerativeAI) â€“ Generates intelligent responses.
    â€¢	LangChain (load_qa_chain) â€“ Manages the retrieval and answering process.
ðŸ”¹ Vector Storage & Retrieval
    â€¢	FAISS (langchain.vectorstores.FAISS) â€“ Stores and retrieves document embeddings efficiently.

**4. Response Structuring Approach**
  The chatbot retrieves relevant content before passing it to Gemini-1.5-Pro for answer generation. The response structure ensures:
    1.	Context-Aware Answers: Uses a prompt to ensure responses stick to the document and avoid hallucinations.
    2.	Structured Output:
          o	If answer is found: The model provides a well-structured, concise response.
          o If no answer is found: It explicitly states: "Answer is not available in the context" instead of generating false information.

**Prompt Template Used**

Prompt_template = """
    Answer the question as detailed as possible from the provided context, make sure to provide all the details. 
    If the answer is not in the provided context, just say, "answer is not available in the context".
    Don't provide incorrect answers.
    Context:\n {context}?\n
    Question: \n{question}\n
    Answer:
"""

**5. Challenges Faced & Solutions Implemented**
   
ðŸ”¸ Challenge 1: Google API Model Not Found (404 Error)
      â€¢	Problem: "models/gemini-pro is not found for API version v1beta".
      â€¢	Solution: Updated the API to use gemini-1.5-pro, ensuring compatibility with the latest version.
ðŸ”¸ Challenge 2: FAISS Serialization Warning
      â€¢	Problem: FAISS raised an error requiring allow_dangerous_deserialization=True.
      â€¢	Solution: Explicitly enabled safe deserialization by verifying the source before loading FAISS:
            Code: new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
ðŸ”¸ Challenge 3: PDF Text Extraction Issues
      â€¢	Problem: Some PDFs didnâ€™t extract text properly due to encoding issues.
      â€¢	Solution: Used extract_text() inside a loop to process each page separately.
ðŸ”¸ Challenge 4: Slow Response Time
      â€¢	Problem: Long texts took too much time to process.
      â€¢	Solution: Optimized chunking strategy using:
          Code: text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)

This ensured better balance between context length and retrieval accuracy.
